"""
ì—ë¸Œë¦¬íƒ€ì„ í¬ë¡¤ëŸ¬ ì‚¬ìš© ì˜ˆì œ
"""

from dotenv import load_dotenv
from everytime_crawler import EverytimeCrawler
import os
import time
from datetime import datetime

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def example_timetable_crawling():
    """ì‹œê°„í‘œ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("=== ì‹œê°„í‘œ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    with EverytimeCrawler() as crawler:
        # WebDriver ì„¤ì • (ë””ë²„ê¹… ì‹œ headless=False ì‚¬ìš©)
        crawler.setup_driver(headless=False)
        
        # ë¡œê·¸ì¸
        if crawler.login():
            # ì‹œê°„í‘œ ìˆ˜ì§‘ (2025ë…„ 1í•™ê¸°)
            timetable_data = crawler.get_timetable(year=2025, semester=1, save_to_file=True)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"ìˆ˜ì§‘ëœ ì‹œê°„í‘œ ìˆ˜: {len(timetable_data)}")
            for i, subject in enumerate(timetable_data[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                print(f"{i}. {subject['subject_name']} - {subject['time']} - {subject['professor']}")

def example_board_crawling():
    """ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("\n=== ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    with EverytimeCrawler() as crawler:
        crawler.setup_driver(headless=False)
        
        if crawler.login():
            # ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ (ì²« 3í˜ì´ì§€)
            board_posts = crawler.get_board_posts("free", page_count=3, save_to_file=True)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ìˆ˜: {len(board_posts)}")
            for i, post in enumerate(board_posts[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                print(f"{i}. {post['title']} - {post['author']} - {post['created_time']}")

def example_specific_board_crawling():
    """íŠ¹ì • ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("\n=== íŠ¹ì • ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    # ì¸ê¸° ê²Œì‹œíŒ IDë“¤
    popular_boards = {
        "free": "ììœ ê²Œì‹œíŒ",
        "secret": "ë¹„ë°€ê²Œì‹œíŒ", 
        "freshman": "ìƒˆë‚´ê¸°ê²Œì‹œíŒ",
        "graduate": "ì¡¸ì—…ìƒê²Œì‹œíŒ",
        "job": "ì·¨ì—…ê²Œì‹œíŒ"
    }
    
    with EverytimeCrawler() as crawler:
        crawler.setup_driver(headless=False)
        
        if crawler.login():
            for board_id, board_name in popular_boards.items():
                print(f"\n{board_name} í¬ë¡¤ë§ ì¤‘...")
                try:
                    posts = crawler.get_board_posts(board_id, page_count=2, save_to_file=True)
                    print(f"{board_name}: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
                except Exception as e:
                    print(f"{board_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

def example_post_detail_crawling():
    """ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("\n=== ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    with EverytimeCrawler() as crawler:
        crawler.setup_driver(headless=False)
        
        if crawler.login():
            # ë¨¼ì € ê²Œì‹œíŒì—ì„œ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            board_posts = crawler.get_board_posts("free", page_count=1, save_to_file=False)
            
            if board_posts:
                # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                first_post = board_posts[0]
                print(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ëŒ€ìƒ: {first_post['title']}")
                
                detail_info = crawler.get_post_detail(first_post['post_link'])
                if detail_info:
                    print(f"ê²Œì‹œê¸€ ë‚´ìš©: {detail_info['content'][:100]}...")
                    print(f"ëŒ“ê¸€ ìˆ˜: {len(detail_info['comments'])}")

def example_massive_board_crawling():
    """ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ (ìµœê·¼ 2ë…„ê°„ ë°ì´í„°)"""
    print("\n=== ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ ===")
    print("âš ï¸ ì£¼ì˜: ì´ ì‘ì—…ì€ ëª‡ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    response = input("ëŒ€ëŸ‰ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("âŒ ëŒ€ëŸ‰ í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    # ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹¤í–‰
    os.system('python examples/massive_board_crawling.py')

def example_specific_date_range_crawling():
    """íŠ¹ì • ê¸°ê°„ ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ"""
    print("\n=== íŠ¹ì • ê¸°ê°„ ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ ===")
    
    with EverytimeCrawler() as crawler:
        crawler.setup_driver(headless=False)
        
        if crawler.login():
            # ì£¼ìš” ê²Œì‹œíŒë§Œ ì„ ë³„ í¬ë¡¤ë§
            priority_boards = ['free', 'secret', 'job']
            
            for board_id in priority_boards:
                board_name = {
                    'free': 'ììœ ê²Œì‹œíŒ',
                    'secret': 'ë¹„ë°€ê²Œì‹œíŒ', 
                    'job': 'ì·¨ì—…ê²Œì‹œíŒ'
                }.get(board_id, board_id)
                
                print(f"\nğŸ“‹ {board_name} ëŒ€ëŸ‰ í¬ë¡¤ë§ ì¤‘...")
                
                # ë§ì€ í˜ì´ì§€ í¬ë¡¤ë§ (ìµœê·¼ 6ê°œì›” ë¶„ëŸ‰ ì¶”ì •)
                posts = crawler.get_board_posts(
                    board_id=board_id,
                    pages=100,  # 100í˜ì´ì§€ (ì•½ 2000ê°œ ê²Œì‹œê¸€)
                    delay=2
                )
                
                if posts:
                    print(f"âœ… {board_name}: {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    # ê²Œì‹œíŒë³„ ê°œë³„ ì €ì¥
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # CSV ì €ì¥
                    csv_filename = f"data/bulk_{board_id}_{timestamp}.csv"
                    crawler.save_board_posts_to_csv(posts, csv_filename)
                    
                    # JSON ì €ì¥
                    json_filename = f"data/bulk_{board_id}_{timestamp}.json"
                    crawler.save_board_posts_to_json(posts, json_filename)
                    
                    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {csv_filename}, {json_filename}")
                else:
                    print(f"âŒ {board_name}: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                
                # ê²Œì‹œíŒ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                print("â³ 10ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(10)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì—ë¸Œë¦¬íƒ€ì„ í¬ë¡¤ëŸ¬ ì˜ˆì œ ì‹¤í–‰")
    print("ì‹¤í–‰í•˜ê¸° ì „ì— .env íŒŒì¼ì— ê³„ì • ì •ë³´ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
    
    # .env íŒŒì¼ í™•ì¸
    if not os.path.exists('.env'):
        print("ê²½ê³ : .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. .env.exampleì„ ì°¸ê³ í•˜ì—¬ .env íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        # ì˜ˆì œ ì‹¤í–‰ ë©”ë‰´
        print("\nì‹¤í–‰í•  ì˜ˆì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì‹œê°„í‘œ í¬ë¡¤ë§")
        print("2. ê¸°ë³¸ ê²Œì‹œíŒ í¬ë¡¤ë§")
        print("3. íŠ¹ì • ê²Œì‹œíŒ í¬ë¡¤ë§")
        print("4. ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§")
        print("5. ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ (2ë…„ì¹˜)")
        print("6. íŠ¹ì • ê¸°ê°„ ëŒ€ëŸ‰ í¬ë¡¤ë§")
        print("0. ëª¨ë“  ì˜ˆì œ ì‹¤í–‰")
        
        choice = input("ì„ íƒ (0-6): ").strip()
        
        if choice == "1":
            example_timetable_crawling()
        elif choice == "2":
            example_board_crawling()
        elif choice == "3":
            example_specific_board_crawling()
        elif choice == "4":
            example_post_detail_crawling()
        elif choice == "5":
            example_massive_board_crawling()
        elif choice == "6":
            example_specific_date_range_crawling()
        elif choice == "0":
            example_timetable_crawling()
            example_board_crawling()
            example_specific_board_crawling()
            example_post_detail_crawling()
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
