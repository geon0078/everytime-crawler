#!/usr/bin/env python3
"""
ëŒ“ê¸€ í¬í•¨ ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from dotenv import load_dotenv
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from everytime_crawler import EverytimeCrawler
import time
import json
import pandas as pd
from datetime import datetime


def massive_board_crawling_with_comments():
    """ëŒ“ê¸€ í¬í•¨ ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§"""
    print("ğŸ¯ ì—ë¸Œë¦¬íƒ€ì„ ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ (ëŒ“ê¸€ í¬í•¨)")
    print("=" * 60)
    
    crawler = EverytimeCrawler()
    
    try:
        print("ğŸ”§ WebDriver ì„¤ì • ì¤‘...")
        crawler.setup_driver(headless=True)  # ëŒ€ëŸ‰ í¬ë¡¤ë§ì€ headless ëª¨ë“œ
        
        print("ğŸ” ë¡œê·¸ì¸ ì‹œë„...")
        if not crawler.login():
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨!")
            return
        
        print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
        
        # í¬ë¡¤ë§í•  ê²Œì‹œíŒ ëª©ë¡
        boards_to_crawl = [
            ("free", "ì„±ë‚¨ìº  ììœ ê²Œì‹œíŒ", 5),     # 5í˜ì´ì§€
            ("secret", "ë¹„ë°€ê²Œì‹œíŒ", 3),        # 3í˜ì´ì§€
            ("graduate", "ì¡¸ì—…ìƒê²Œì‹œíŒ", 2),    # 2í˜ì´ì§€
            ("freshman", "ìƒˆë‚´ê¸°ê²Œì‹œíŒ", 2),    # 2í˜ì´ì§€
        ]
        
        all_results = {}
        
        for board_id, board_name, pages in boards_to_crawl:
            print(f"\nğŸ“‹ {board_name} í¬ë¡¤ë§ ì‹œì‘...")
            
            try:
                # ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
                posts = crawler.get_board_posts(board_id, pages=pages, delay=3)
                
                if posts:
                    print(f"âœ… {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì„±ê³µ!")
                    
                    # ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                    print("ğŸ’¬ ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                    detailed_posts = []
                    
                    posts_with_comments = [p for p in posts if int(p.get('comment_count', '0')) > 0]
                    print(f"ğŸ“Š ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€: {len(posts_with_comments)}ê°œ")
                    
                    # ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ë§Œ ìƒì„¸ í¬ë¡¤ë§ (ìµœëŒ€ 10ê°œ)
                    for i, post in enumerate(posts_with_comments[:10]):
                        if post.get('post_link'):
                            print(f"   ğŸ“– {i+1}/{min(10, len(posts_with_comments))} ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                            detail = crawler.get_post_detail(post['post_link'])
                            if detail:
                                combined_post = post.copy()
                                combined_post.update({
                                    'full_content': detail.get('content', ''),
                                    'comments': detail.get('comments', []),
                                    'detailed_comment_count': detail.get('comment_count', 0)
                                })
                                detailed_posts.append(combined_post)
                                
                                comment_count = len(detail.get('comments', []))
                                print(f"     ğŸ’¬ ëŒ“ê¸€ {comment_count}ê°œ ìˆ˜ì§‘")
                            
                            time.sleep(2)  # ìš”ì²­ ê°„ ëŒ€ê¸°
                    
                    # ë°ì´í„° ì €ì¥
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # ì „ì²´ ê²Œì‹œê¸€ ëª©ë¡
                    df = pd.DataFrame(posts)
                    csv_file = f"data/massive_{board_id}_{timestamp}.csv"
                    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                    
                    json_file = f"data/massive_{board_id}_{timestamp}.json"
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(posts, f, ensure_ascii=False, indent=2)
                    
                    # ëŒ“ê¸€ í¬í•¨ ìƒì„¸ ì •ë³´
                    if detailed_posts:
                        detailed_csv = f"data/massive_{board_id}_detailed_{timestamp}.csv"
                        detailed_df = pd.DataFrame([
                            {
                                'title': p.get('title', ''),
                                'content': p.get('content', ''),
                                'full_content': p.get('full_content', ''),
                                'author': p.get('author', ''),
                                'created_time': p.get('created_time', ''),
                                'comment_count': p.get('comment_count', ''),
                                'detailed_comment_count': p.get('detailed_comment_count', 0),
                                'post_link': p.get('post_link', ''),
                                'comments_json': json.dumps(p.get('comments', []), ensure_ascii=False),
                                'board_id': board_id,
                                'board_name': board_name
                            } for p in detailed_posts
                        ])
                        detailed_df.to_csv(detailed_csv, index=False, encoding='utf-8-sig')
                        
                        detailed_json = f"data/massive_{board_id}_detailed_{timestamp}.json"
                        with open(detailed_json, 'w', encoding='utf-8') as f:
                            json.dump(detailed_posts, f, ensure_ascii=False, indent=2)
                        
                        print(f"ğŸ’¾ ìƒì„¸ì •ë³´ ì €ì¥: {len(detailed_posts)}ê°œ ê²Œì‹œê¸€")
                    
                    # ê²°ê³¼ ìš”ì•½
                    all_results[board_id] = {
                        'board_name': board_name,
                        'total_posts': len(posts),
                        'posts_with_comments': len(posts_with_comments),
                        'detailed_posts': len(detailed_posts),
                        'total_comments': sum(len(p.get('comments', [])) for p in detailed_posts)
                    }
                    
                    print(f"ğŸ“Š {board_name} ìˆ˜ì§‘ ì™„ë£Œ:")
                    print(f"   - ì „ì²´ ê²Œì‹œê¸€: {len(posts)}ê°œ")
                    print(f"   - ëŒ“ê¸€ ìˆëŠ” ê²Œì‹œê¸€: {len(posts_with_comments)}ê°œ")  
                    print(f"   - ìƒì„¸ ìˆ˜ì§‘ ê²Œì‹œê¸€: {len(detailed_posts)}ê°œ")
                    print(f"   - ì´ ìˆ˜ì§‘ ëŒ“ê¸€: {sum(len(p.get('comments', [])) for p in detailed_posts)}ê°œ")
                    
                else:
                    print(f"âŒ {board_name}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                print(f"âŒ {board_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
            
            print("â³ 10ì´ˆ ëŒ€ê¸°...")
            time.sleep(10)
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        print("\n" + "="*60)
        print("ğŸ‰ ëŒ€ëŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì „ì²´ ê²°ê³¼ ìš”ì•½:")
        print("="*60)
        
        total_posts = 0
        total_comments = 0
        
        for board_id, result in all_results.items():
            print(f"ğŸ“‹ {result['board_name']}:")
            print(f"   - ê²Œì‹œê¸€: {result['total_posts']}ê°œ")
            print(f"   - ëŒ“ê¸€: {result['total_comments']}ê°œ")
            
            total_posts += result['total_posts']
            total_comments += result['total_comments']
        
        print("\nğŸ“Š ì´í•©:")
        print(f"   - ì „ì²´ ê²Œì‹œê¸€: {total_posts}ê°œ")
        print(f"   - ì „ì²´ ëŒ“ê¸€: {total_comments}ê°œ")
        
        # ìš”ì•½ ì •ë³´ ì €ì¥
        summary = {
            'crawl_completed_at': datetime.now().isoformat(),
            'boards': all_results,
            'totals': {
                'total_posts': total_posts,
                'total_comments': total_comments
            }
        }
        
        summary_file = f"data/crawling_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“‹ í¬ë¡¤ë§ ìš”ì•½ ì €ì¥: {summary_file}")
        
    except Exception as e:
        print(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if crawler.driver:
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ...")
            crawler.quit()
        
        print("\nâœ… ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    massive_board_crawling_with_comments()
