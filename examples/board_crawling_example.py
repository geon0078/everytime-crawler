"""
ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ
"""

import sys
import os
from dotenv import load_dotenv
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from everytime_crawler import EverytimeCrawler
import time


def main():
    """ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ ì‹¤í–‰"""
    print("ğŸ¯ ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ í¬ë¡¤ë§ ì˜ˆì œ ì‹œì‘")
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = EverytimeCrawler()
    
    try:
        # WebDriver ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™”ë¡œ ë””ë²„ê¹… ê°€ëŠ¥)
        crawler.setup_driver(headless=False)
        
        # ë¡œê·¸ì¸
        print("\nğŸ” ë¡œê·¸ì¸ ì‹œë„...")
        if crawler.login():
            print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            
            # ê²Œì‹œíŒ ëª©ë¡
            boards_to_crawl = [
                ("free", "ììœ ê²Œì‹œíŒ", 2),
                ("secret", "ë¹„ë°€ê²Œì‹œíŒ", 1),
                ("freshman", "ìƒˆë‚´ê¸°ê²Œì‹œíŒ", 1)
            ]
            
            for board_id, board_name, pages in boards_to_crawl:
                print(f"\nğŸ“‹ {board_name} í¬ë¡¤ë§ ì‹œì‘...")
                
                # ê²Œì‹œê¸€ ëª©ë¡ í¬ë¡¤ë§
                posts = crawler.get_board_posts(
                    board_id=board_id,
                    pages=pages,
                    delay=3  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                )
                
                if posts:
                    print(f"\nğŸ“Š {board_name} í¬ë¡¤ë§ ê²°ê³¼:")
                    print(f"   ì´ ê²Œì‹œê¸€ ìˆ˜: {len(posts)}ê°œ")
                    
                    # ì²« 3ê°œ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°
                    print("\nğŸ“ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°:")
                    for i, post in enumerate(posts[:3], 1):
                        print(f"   {i}. ì œëª©: {post.get('title', 'N/A')}")
                        print(f"      ì‘ì„±ì: {post.get('author', 'N/A')}")
                        print(f"      ì‹œê°„: {post.get('created_time', 'N/A')}")
                        print(f"      ëŒ“ê¸€: {post.get('comment_count', '0')}ê°œ")
                        if post.get('post_link'):
                            print(f"      ë§í¬: {post['post_link']}")
                        print()
                    
                    # CSV íŒŒì¼ë¡œ ì €ì¥
                    crawler.save_board_posts_to_csv(posts)
                    
                    # JSON íŒŒì¼ë¡œ ì €ì¥
                    crawler.save_board_posts_to_json(posts)
                    
                    # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì˜ˆì‹œ)
                    if posts and posts[0].get('post_link'):
                        print(f"\nğŸ“– ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í™•ì¸...")
                        detail = crawler.get_post_detail(posts[0]['post_link'])
                        if detail:
                            print(f"   ë‚´ìš© ê¸¸ì´: {len(detail.get('content', ''))}ì")
                            print(f"   ëŒ“ê¸€ ìˆ˜: {detail.get('comment_count', 0)}ê°œ")
                else:
                    print(f"âŒ {board_name}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ê²Œì‹œíŒ ê°„ ëŒ€ê¸°
                time.sleep(5)
        
        else:
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨!")
            return
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        if crawler.driver:
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ...")
            crawler.quit()
        
        print("\nâœ… ê²Œì‹œíŒ í¬ë¡¤ë§ ì™„ë£Œ!")


def demo_board_analysis():
    """í¬ë¡¤ë§ëœ ê²Œì‹œíŒ ë°ì´í„° ë¶„ì„ ì˜ˆì œ"""
    print("\nğŸ“Š ê²Œì‹œíŒ ë°ì´í„° ë¶„ì„ ì˜ˆì œ")
    
    import pandas as pd
    import glob
    
    # ì €ì¥ëœ CSV íŒŒì¼ ì°¾ê¸°
    csv_files = glob.glob("data/board_*.csv")
    
    if not csv_files:
        print("âŒ ë¶„ì„í•  ê²Œì‹œíŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    for csv_file in csv_files:
        print(f"\nğŸ“‹ íŒŒì¼ ë¶„ì„: {csv_file}")
        
        try:
            df = pd.read_csv(csv_file)
            
            print(f"   ì´ ê²Œì‹œê¸€ ìˆ˜: {len(df)}")
            print(f"   ì»¬ëŸ¼: {list(df.columns)}")
            
            # ì‘ì„±ìë³„ ê²Œì‹œê¸€ ìˆ˜
            if 'author' in df.columns:
                author_counts = df['author'].value_counts().head(5)
                print(f"\n   ìƒìœ„ ì‘ì„±ì:")
                for author, count in author_counts.items():
                    print(f"     {author}: {count}ê°œ")
            
            # ëŒ“ê¸€ ìˆ˜ í†µê³„
            if 'comment_count' in df.columns:
                df['comment_count'] = pd.to_numeric(df['comment_count'], errors='coerce').fillna(0)
                avg_comments = df['comment_count'].mean()
                max_comments = df['comment_count'].max()
                print(f"\n   ëŒ“ê¸€ í†µê³„:")
                print(f"     í‰ê·  ëŒ“ê¸€ ìˆ˜: {avg_comments:.1f}ê°œ")
                print(f"     ìµœëŒ€ ëŒ“ê¸€ ìˆ˜: {max_comments}ê°œ")
                
                # ëŒ“ê¸€ì´ ë§ì€ ê²Œì‹œê¸€
                top_commented = df.nlargest(3, 'comment_count')
                print(f"\n   ëŒ“ê¸€ ë§ì€ ê²Œì‹œê¸€:")
                for _, post in top_commented.iterrows():
                    print(f"     {post.get('title', 'N/A')[:50]}... ({post['comment_count']}ê°œ)")
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    print("ğŸš€ ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰")
    print("=" * 50)
    
    # ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰
    main()
    
    # ë°ì´í„° ë¶„ì„ ì˜ˆì œ
    demo_board_analysis()
    
    print("\n" + "=" * 50)
    print("âœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
