"""
ì—ë¸Œë¦¬íƒ€ì„ ëŒ€ëŸ‰ ê²Œì‹œíŒ ë°ì´í„° í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
ìµœê·¼ 2ë…„ê°„ì˜ ëª¨ë“  ê²Œì‹œíŒ ë°ì´í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜ì§‘
"""

import sys
import os
from dotenv import load_dotenv
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from everytime_crawler import EverytimeCrawler, BOARD_MAP
import time
import json
import pandas as pd
from datetime import datetime, timedelta
import threading
import queue
import signal


class MassiveBoardCrawler:
    """ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.crawler = None
        self.total_posts = 0
        self.total_boards = 0
        self.failed_boards = []
        self.success_boards = []
        self.start_time = None
        self.stop_crawling = False
        
        # ì•ˆì „í•œ ì¢…ë£Œë¥¼ ìœ„í•œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """ì•ˆì „í•œ ì¢…ë£Œë¥¼ ìœ„í•œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
        print(f"\nâš ï¸ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€ (Signal {signum})")
        print("ì•ˆì „í•œ ì¢…ë£Œë¥¼ ìœ„í•´ í˜„ì¬ ì‘ì—…ì„ ì™„ë£Œí•˜ëŠ” ì¤‘...")
        self.stop_crawling = True
    
    def estimate_crawling_scope(self):
        """í¬ë¡¤ë§ ë²”ìœ„ ì¶”ì •"""
        print("ğŸ“Š í¬ë¡¤ë§ ë²”ìœ„ ì¶”ì •")
        print("=" * 60)
        
        # ê²Œì‹œíŒë³„ ì˜ˆìƒ í˜ì´ì§€ ìˆ˜ (ê²½í—˜ì  ì¶”ì •)
        board_estimates = {
            "free": 1000,      # ììœ ê²Œì‹œíŒ - ê°€ì¥ í™œë°œ
            "secret": 800,     # ë¹„ë°€ê²Œì‹œíŒ
            "freshman": 300,   # ìƒˆë‚´ê¸°ê²Œì‹œíŒ
            "graduate": 200,   # ì¡¸ì—…ìƒê²Œì‹œíŒ
            "job": 500,        # ì·¨ì—…ê²Œì‹œíŒ
            "exam": 400,       # ì‹œí—˜ì •ë³´ê²Œì‹œíŒ
            "club": 150,       # ë™ì•„ë¦¬ê²Œì‹œíŒ
            "market": 250      # ì¥í„°ê²Œì‹œíŒ
        }
        
        total_estimated_pages = sum(board_estimates.values())
        total_estimated_posts = total_estimated_pages * 20  # í˜ì´ì§€ë‹¹ ì•½ 20ê°œ ê²Œì‹œê¸€
        
        print(f"ğŸ“‹ ëŒ€ìƒ ê²Œì‹œíŒ: {len(BOARD_MAP)}ê°œ")
        print(f"ğŸ“„ ì˜ˆìƒ ì´ í˜ì´ì§€: {total_estimated_pages:,}í˜ì´ì§€")
        print(f"ğŸ“ ì˜ˆìƒ ì´ ê²Œì‹œê¸€: {total_estimated_posts:,}ê°œ")
        
        # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚° (í˜ì´ì§€ë‹¹ 3ì´ˆ ê°€ì •)
        estimated_seconds = total_estimated_pages * 3
        estimated_hours = estimated_seconds / 3600
        
        print(f"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_hours:.1f}ì‹œê°„")
        print(f"ğŸ’¾ ì˜ˆìƒ ë°ì´í„° í¬ê¸°: {total_estimated_posts * 0.5 / 1024:.1f}MB")
        
        print("\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
        print("- ì´ëŠ” ì¶”ì •ì¹˜ì´ë©° ì‹¤ì œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print("- ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ì ì ˆí•œ ëŒ€ê¸°ì‹œê°„ì„ ì„¤ì •í•©ë‹ˆë‹¤")
        print("- ì–¸ì œë“ ì§€ Ctrl+Cë¡œ ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        return board_estimates
    
    def crawl_massive_board_data(self, 
                                target_boards=None, 
                                max_pages_per_board=None,
                                delay_between_pages=3,
                                delay_between_boards=10,
                                save_interval=50):
        """
        ëŒ€ëŸ‰ ê²Œì‹œíŒ ë°ì´í„° í¬ë¡¤ë§
        
        Args:
            target_boards (list): í¬ë¡¤ë§í•  ê²Œì‹œíŒ ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ê²Œì‹œíŒ)
            max_pages_per_board (int): ê²Œì‹œíŒë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            delay_between_pages (int): í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
            delay_between_boards (int): ê²Œì‹œíŒ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
            save_interval (int): ëª‡ ê°œ ê²Œì‹œê¸€ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥í• ì§€
        """
        
        if target_boards is None:
            target_boards = list(BOARD_MAP.keys())
        
        if max_pages_per_board is None:
            max_pages_per_board = 1000  # ì•ˆì „í•œ ê¸°ë³¸ê°’
        
        print(f"ğŸš€ ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“‹ ëŒ€ìƒ ê²Œì‹œíŒ: {len(target_boards)}ê°œ")
        print(f"ğŸ“„ ê²Œì‹œíŒë‹¹ ìµœëŒ€ í˜ì´ì§€: {max_pages_per_board}")
        print(f"â±ï¸ í˜ì´ì§€ ê°„ ëŒ€ê¸°: {delay_between_pages}ì´ˆ")
        print(f"â±ï¸ ê²Œì‹œíŒ ê°„ ëŒ€ê¸°: {delay_between_boards}ì´ˆ")
        print("=" * 60)
        
        self.start_time = datetime.now()
        self.crawler = EverytimeCrawler()
        
        try:
            # WebDriver ì„¤ì • (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ë¦¬ì†ŒìŠ¤ ì ˆì•½)
            self.crawler.setup_driver(headless=True)
            
            # ë¡œê·¸ì¸
            if not self.crawler.login():
                print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨! í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return
            
            print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            
            # ê° ê²Œì‹œíŒë³„ í¬ë¡¤ë§
            for board_idx, board_id in enumerate(target_boards, 1):
                if self.stop_crawling:
                    print("\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                
                board_name = BOARD_MAP.get(board_id, board_id)
                print(f"\nğŸ“‹ [{board_idx}/{len(target_boards)}] {board_name} í¬ë¡¤ë§ ì‹œì‘...")
                
                try:
                    # ê²Œì‹œíŒë³„ í¬ë¡¤ë§ ì‹¤í–‰
                    board_posts = self._crawl_single_board_comprehensive(
                        board_id, 
                        max_pages_per_board, 
                        delay_between_pages,
                        save_interval
                    )
                    
                    if board_posts:
                        self.success_boards.append({
                            'board_id': board_id,
                            'board_name': board_name,
                            'post_count': len(board_posts),
                            'completed_at': datetime.now().isoformat()
                        })
                        
                        self.total_posts += len(board_posts)
                        print(f"âœ… {board_name} ì™„ë£Œ: {len(board_posts)}ê°œ ê²Œì‹œê¸€")
                        
                        # ê²Œì‹œíŒë³„ ê²°ê³¼ ì €ì¥
                        self._save_board_results(board_id, board_posts)
                        
                    else:
                        print(f"âŒ {board_name}: ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        self.failed_boards.append({
                            'board_id': board_id,
                            'board_name': board_name,
                            'error': 'No posts found'
                        })
                
                except Exception as e:
                    print(f"âŒ {board_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    self.failed_boards.append({
                        'board_id': board_id,
                        'board_name': board_name,
                        'error': str(e)
                    })
                
                # ê²Œì‹œíŒ ê°„ ëŒ€ê¸° (ë§ˆì§€ë§‰ ê²Œì‹œíŒì´ ì•„ë‹Œ ê²½ìš°)
                if board_idx < len(target_boards) and not self.stop_crawling:
                    print(f"â³ {delay_between_boards}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay_between_boards)
            
            # ìµœì¢… ê²°ê³¼ ì €ì¥
            self._save_final_summary()
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            if self.crawler and self.crawler.driver:
                self.crawler.quit()
            
            self._print_final_statistics()
    
    def _crawl_single_board_comprehensive(self, board_id, max_pages, delay, save_interval):
        """ë‹¨ì¼ ê²Œì‹œíŒì˜ í¬ê´„ì  í¬ë¡¤ë§"""
        all_posts = []
        page = 1
        consecutive_empty_pages = 0
        max_empty_pages = 5  # ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ 5ê°œ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
        
        print(f"   ğŸ“„ í˜ì´ì§€ë³„ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        while page <= max_pages and consecutive_empty_pages < max_empty_pages:
            if self.stop_crawling:
                break
            
            try:
                # í˜„ì¬ í˜ì´ì§€ í¬ë¡¤ë§
                page_posts = self.crawler.get_board_posts(
                    board_id=board_id,
                    pages=1,  # í•œ í˜ì´ì§€ì”© ì²˜ë¦¬
                    delay=delay
                )
                
                if page_posts:
                    all_posts.extend(page_posts)
                    consecutive_empty_pages = 0
                    print(f"     í˜ì´ì§€ {page}: {len(page_posts)}ê°œ ê²Œì‹œê¸€")
                    
                    # ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
                    if len(all_posts) % save_interval == 0:
                        self._save_intermediate_results(board_id, all_posts[-save_interval:])
                    
                else:
                    consecutive_empty_pages += 1
                    print(f"     í˜ì´ì§€ {page}: ë¹ˆ í˜ì´ì§€ ({consecutive_empty_pages}/{max_empty_pages})")
                
                # 2ë…„ì¹˜ ë°ì´í„°ì¸ì§€ í™•ì¸ (ë‚ ì§œ ê¸°ë°˜ ì¤‘ë‹¨)
                if self._should_stop_by_date(page_posts):
                    print(f"     ğŸ“… 2ë…„ ì´ì „ ë°ì´í„° ë„ë‹¬, í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                
                page += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if page % 10 == 0:
                    elapsed = datetime.now() - self.start_time
                    print(f"     ğŸ“Š ì§„í–‰: {page}í˜ì´ì§€, ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€, ê²½ê³¼ì‹œê°„: {elapsed}")
                
            except Exception as e:
                print(f"     âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                consecutive_empty_pages += 1
                page += 1
                continue
        
        print(f"   âœ… ê²Œì‹œíŒ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€")
        return all_posts
    
    def _should_stop_by_date(self, posts):
        """ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨ ì—¬ë¶€ ê²°ì •"""
        if not posts:
            return False
        
        # 2ë…„ ì „ ë‚ ì§œ ê³„ì‚°
        two_years_ago = datetime.now() - timedelta(days=730)
        
        for post in posts:
            created_time = post.get('created_time', '')
            if created_time:
                try:
                    # ì—ë¸Œë¦¬íƒ€ì„ ë‚ ì§œ í˜•ì‹ íŒŒì‹± ì‹œë„
                    # ì˜ˆ: "07/02", "2023/07/02" ë“±
                    if '/' in created_time:
                        parts = created_time.split('/')
                        if len(parts) == 2:  # "MM/DD" í˜•ì‹
                            # í˜„ì¬ ì—°ë„ ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±
                            current_year = datetime.now().year
                            month, day = int(parts[0]), int(parts[1])
                            post_date = datetime(current_year, month, day)
                            
                            # ë§Œì•½ ë¯¸ë˜ ë‚ ì§œë¼ë©´ ì‘ë…„ ë°ì´í„°ë¡œ ê°„ì£¼
                            if post_date > datetime.now():
                                post_date = datetime(current_year - 1, month, day)
                            
                        elif len(parts) == 3:  # "YYYY/MM/DD" í˜•ì‹
                            year, month, day = int(parts[0]), int(parts[1]), int(parts[2])
                            post_date = datetime(year, month, day)
                        else:
                            continue
                        
                        # 2ë…„ ì´ì „ ë°ì´í„°ì¸ì§€ í™•ì¸
                        if post_date < two_years_ago:
                            return True
                            
                except (ValueError, IndexError):
                    continue
        
        return False
    
    def _save_intermediate_results(self, board_id, posts):
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/intermediate_{board_id}_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"     âš ï¸ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_board_results(self, board_id, posts):
        """ê²Œì‹œíŒë³„ ìµœì¢… ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_filename = f"data/massive_crawl_{board_id}_{timestamp}.json"
        try:
            with open(json_filename, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
            print(f"     ğŸ’¾ JSON ì €ì¥: {json_filename}")
        except Exception as e:
            print(f"     âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # CSV ì €ì¥
        csv_filename = f"data/massive_crawl_{board_id}_{timestamp}.csv"
        try:
            df = pd.DataFrame(posts)
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"     ğŸ’¾ CSV ì €ì¥: {csv_filename}")
        except Exception as e:
            print(f"     âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _save_final_summary(self):
        """ìµœì¢… í¬ë¡¤ë§ ìš”ì•½ ì €ì¥"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        summary = {
            'crawling_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration.total_seconds(),
                'duration_formatted': str(duration)
            },
            'statistics': {
                'total_posts': self.total_posts,
                'total_boards_attempted': len(BOARD_MAP),
                'successful_boards': len(self.success_boards),
                'failed_boards': len(self.failed_boards)
            },
            'successful_boards': self.success_boards,
            'failed_boards': self.failed_boards
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_filename = f"data/massive_crawl_summary_{timestamp}.json"
        
        try:
            with open(summary_filename, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“Š í¬ë¡¤ë§ ìš”ì•½ ì €ì¥: {summary_filename}")
        except Exception as e:
            print(f"\nâŒ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _print_final_statistics(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        end_time = datetime.now()
        duration = end_time - self.start_time if self.start_time else timedelta(0)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š ëŒ€ëŸ‰ í¬ë¡¤ë§ ì™„ë£Œ í†µê³„")
        print("=" * 60)
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {duration}")
        print(f"ğŸ“ ì´ ìˆ˜ì§‘ ê²Œì‹œê¸€: {self.total_posts:,}ê°œ")
        print(f"âœ… ì„±ê³µí•œ ê²Œì‹œíŒ: {len(self.success_boards)}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ ê²Œì‹œíŒ: {len(self.failed_boards)}ê°œ")
        
        if self.success_boards:
            print("\nâœ… ì„±ê³µí•œ ê²Œì‹œíŒ:")
            for board in self.success_boards:
                print(f"   {board['board_name']}: {board['post_count']:,}ê°œ")
        
        if self.failed_boards:
            print("\nâŒ ì‹¤íŒ¨í•œ ê²Œì‹œíŒ:")
            for board in self.failed_boards:
                print(f"   {board['board_name']}: {board['error']}")
        
        if self.total_posts > 0:
            avg_time_per_post = duration.total_seconds() / self.total_posts
            print(f"\nğŸ“ˆ í‰ê·  ê²Œì‹œê¸€ë‹¹ ì†Œìš”ì‹œê°„: {avg_time_per_post:.2f}ì´ˆ")
        
        print("\nğŸ’¾ ì €ì¥ëœ íŒŒì¼ë“¤:")
        print("   data/massive_crawl_*.json - ê²Œì‹œíŒë³„ JSON ë°ì´í„°")
        print("   data/massive_crawl_*.csv - ê²Œì‹œíŒë³„ CSV ë°ì´í„°")
        print("   data/massive_crawl_summary_*.json - í¬ë¡¤ë§ ìš”ì•½")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì—ë¸Œë¦¬íƒ€ì„ ëŒ€ëŸ‰ ê²Œì‹œíŒ í¬ë¡¤ë§")
    print("ìµœê·¼ 2ë…„ê°„ì˜ ëª¨ë“  ê²Œì‹œíŒ ë°ì´í„° ìˆ˜ì§‘")
    print("=" * 60)
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not os.path.exists('.env'):
        print("âŒ .env íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. .env.exampleì„ ì°¸ê³ í•˜ì—¬ .env íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
        return
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    massive_crawler = MassiveBoardCrawler()
    
    # í¬ë¡¤ë§ ë²”ìœ„ ì¶”ì •
    board_estimates = massive_crawler.estimate_crawling_scope()
    
    # ì‚¬ìš©ì í™•ì¸
    print("\n" + "=" * 60)
    response = input("ëŒ€ëŸ‰ í¬ë¡¤ë§ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    
    if response in ['y', 'yes']:
        print("\nğŸš€ ëŒ€ëŸ‰ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("ì–¸ì œë“ ì§€ Ctrl+Cë¥¼ ëˆŒëŸ¬ ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        massive_crawler.crawl_massive_board_data(
            target_boards=None,  # ëª¨ë“  ê²Œì‹œíŒ
            max_pages_per_board=500,  # ê²Œì‹œíŒë‹¹ ìµœëŒ€ 500í˜ì´ì§€ (2ë…„ì¹˜ ì¶”ì •)
            delay_between_pages=3,  # í˜ì´ì§€ ê°„ 3ì´ˆ ëŒ€ê¸°
            delay_between_boards=10,  # ê²Œì‹œíŒ ê°„ 10ì´ˆ ëŒ€ê¸°
            save_interval=100  # 100ê°œ ê²Œì‹œê¸€ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        )
        
    else:
        print("âŒ í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")


def quick_test_crawling():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© í¬ë¡¤ë§ (ì†ŒëŸ‰ ë°ì´í„°)"""
    print("ğŸ§ª ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ (ê° ê²Œì‹œíŒ 5í˜ì´ì§€ì”©)")
    
    massive_crawler = MassiveBoardCrawler()
    
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
    massive_crawler.crawl_massive_board_data(
        target_boards=['free', 'secret'],  # 2ê°œ ê²Œì‹œíŒë§Œ
        max_pages_per_board=5,  # ê° 5í˜ì´ì§€ë§Œ
        delay_between_pages=1,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        delay_between_boards=3,
        save_interval=20
    )


if __name__ == "__main__":
    print("ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì „ì²´ ëŒ€ëŸ‰ í¬ë¡¤ë§ (2ë…„ì¹˜)")
    print("2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§")
    
    choice = input("ì„ íƒ (1/2): ").strip()
    
    if choice == "1":
        main()
    elif choice == "2":
        quick_test_crawling()
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
