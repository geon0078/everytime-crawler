#!/usr/bin/env python3
"""
2025ë…„ 7ì›” ììœ ê²Œì‹œíŒ ì „ì²´ ê¸€ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from dotenv import load_dotenv
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from everytime_crawler import EverytimeCrawler
import time
import json
import pandas as pd
from datetime import datetime, timedelta
import re


def parse_everytime_time(time_str):
    """ì—ë¸Œë¦¬íƒ€ì„ ì‹œê°„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    if not time_str:
        return None
    
    now = datetime.now()
    
    # "3ë¶„ ì „", "17ë¶„ ì „" í˜•íƒœ
    if "ë¶„ ì „" in time_str:
        minutes = int(re.findall(r'\d+', time_str)[0])
        return now - timedelta(minutes=minutes)
    
    # "1ì‹œê°„ ì „", "2ì‹œê°„ ì „" í˜•íƒœ
    elif "ì‹œê°„ ì „" in time_str:
        hours = int(re.findall(r'\d+', time_str)[0])
        return now - timedelta(hours=hours)
    
    # "07/01 09:11", "20:26" í˜•íƒœ
    elif "/" in time_str:
        # "07/01 09:11" í˜•íƒœ
        try:
            month_day, time_part = time_str.split(" ")
            month, day = month_day.split("/")
            hour, minute = time_part.split(":")
            
            # 2025ë…„ìœ¼ë¡œ ê°€ì •
            return datetime(2025, int(month), int(day), int(hour), int(minute))
        except:
            pass
    
    elif ":" in time_str and len(time_str) == 5:
        # "20:26" í˜•íƒœ (ì˜¤ëŠ˜)
        try:
            hour, minute = time_str.split(":")
            today = now.date()
            return datetime.combine(today, datetime.strptime(f"{hour}:{minute}", "%H:%M").time())
        except:
            pass
    
    return None


def is_july_2025(time_str):
    """ì£¼ì–´ì§„ ì‹œê°„ ë¬¸ìì—´ì´ 2025ë…„ 7ì›”ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸"""
    parsed_time = parse_everytime_time(time_str)
    if parsed_time:
        return parsed_time.year == 2025 and parsed_time.month == 7
    
    # íŒŒì‹±ì— ì‹¤íŒ¨í•œ ê²½ìš° ë¬¸ìì—´ë¡œ íŒë‹¨
    if "07/" in time_str:  # 07/01, 07/02 ë“±
        return True
    
    # ìµœê·¼ ê¸€ (ë¶„ ì „, ì‹œê°„ ì „)ë„ 7ì›”ë¡œ ê°„ì£¼ (í˜„ì¬ê°€ 7ì›”ì´ë¯€ë¡œ)
    if "ë¶„ ì „" in time_str or "ì‹œê°„ ì „" in time_str:
        return True
    
    # ì˜¤ëŠ˜ ì‘ì„±ëœ ê¸€ (ì‹œ:ë¶„ í˜•íƒœ)ë„ 7ì›”ë¡œ ê°„ì£¼
    if ":" in time_str and len(time_str) == 5:
        return True
    
    return False


def crawl_july_2025_free_board():
    """2025ë…„ 7ì›” ììœ ê²Œì‹œíŒ ì „ì²´ ê¸€ í¬ë¡¤ë§"""
    print("ğŸ—“ï¸ 2025ë…„ 7ì›” ììœ ê²Œì‹œíŒ ì „ì²´ ê¸€ í¬ë¡¤ë§")
    print("=" * 60)
    
    crawler = EverytimeCrawler()
    all_july_posts = []
    july_detailed_posts = []
    
    try:
        print("ğŸ”§ WebDriver ì„¤ì • ì¤‘...")
        crawler.setup_driver(headless=True)  # ëŒ€ëŸ‰ í¬ë¡¤ë§ì€ headless ëª¨ë“œ
        
        print("ğŸ” ë¡œê·¸ì¸ ì‹œë„...")
        if not crawler.login():
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨!")
            return
        
        print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
        print("\nğŸ“‹ ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì¶©ë¶„í•œ í˜ì´ì§€ ìˆ˜ë¡œ í¬ë¡¤ë§ (7ì›” ì „ì²´ ë°ì´í„° í™•ë³´)
        max_pages = 50  # í•„ìš”ì— ë”°ë¼ ì¡°ì •
        current_page = 1
        consecutive_non_july = 0  # ì—°ì†ìœ¼ë¡œ 7ì›”ì´ ì•„ë‹Œ ê¸€ ìˆ˜
        
        while current_page <= max_pages:
            print(f"\nğŸ“„ í˜ì´ì§€ {current_page} í¬ë¡¤ë§ ì¤‘...")
            
            try:
                # í•œ í˜ì´ì§€ì”© í¬ë¡¤ë§
                posts = crawler.get_board_posts("free", pages=1, delay=3)
                
                if not posts:
                    print(f"âŒ í˜ì´ì§€ {current_page}ì—ì„œ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                page_july_posts = []
                page_non_july_posts = 0
                
                for post in posts:
                    created_time = post.get('created_time', '')
                    
                    if is_july_2025(created_time):
                        page_july_posts.append(post)
                        consecutive_non_july = 0  # 7ì›” ê¸€ì„ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
                    else:
                        page_non_july_posts += 1
                        consecutive_non_july += 1
                
                all_july_posts.extend(page_july_posts)
                
                print(f"   ğŸ“… 7ì›” ê²Œì‹œê¸€: {len(page_july_posts)}ê°œ")
                print(f"   ğŸ“Š ëˆ„ì  7ì›” ê²Œì‹œê¸€: {len(all_july_posts)}ê°œ")
                
                # ì—°ì†ìœ¼ë¡œ 50ê°œ ì´ìƒì˜ ë¹„-7ì›” ê¸€ì´ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
                if consecutive_non_july >= 50:
                    print(f"\nâ¹ï¸ ì—°ì†ìœ¼ë¡œ 7ì›”ì´ ì•„ë‹Œ ê¸€ì´ {consecutive_non_july}ê°œ ë‚˜ì™”ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                # í˜ì´ì§€ì— 7ì›” ê¸€ì´ í•˜ë‚˜ë„ ì—†ê³ , ì´ë¯¸ ì¶©ë¶„í•œ 7ì›” ê¸€ì„ ìˆ˜ì§‘í–ˆë‹¤ë©´ ì¤‘ë‹¨
                if len(page_july_posts) == 0 and len(all_july_posts) > 100:
                    print(f"\nâ¹ï¸ 7ì›” ê¸€ì´ ë” ì´ìƒ ì—†ìœ¼ë¯€ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                current_page += 1
                time.sleep(3)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {current_page} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                current_page += 1
                time.sleep(5)
                continue
        
        print(f"\nğŸ‰ 7ì›” ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_july_posts)}ê°œ")
        
        if not all_july_posts:
            print("âŒ 7ì›” ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        print("\nğŸ’¬ ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        posts_with_comments = [p for p in all_july_posts if int(p.get('comment_count', '0')) > 0]
        print(f"ğŸ“Š ëŒ“ê¸€ì´ ìˆëŠ” 7ì›” ê²Œì‹œê¸€: {len(posts_with_comments)}ê°œ")
        
        # ëŒ“ê¸€ì´ ìˆëŠ” ê²Œì‹œê¸€ ì¤‘ ìµœëŒ€ 30ê°œ ìƒì„¸ í¬ë¡¤ë§
        max_detailed = min(30, len(posts_with_comments))
        for i, post in enumerate(posts_with_comments[:max_detailed]):
            if post.get('post_link'):
                print(f"   ğŸ“– {i+1}/{max_detailed} ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                detail = crawler.get_post_detail(post['post_link'])
                if detail:
                    combined_post = post.copy()
                    combined_post.update({
                        'full_content': detail.get('content', ''),
                        'comments': detail.get('comments', []),
                        'detailed_comment_count': detail.get('comment_count', 0)
                    })
                    july_detailed_posts.append(combined_post)
                    
                    comment_count = len(detail.get('comments', []))
                    print(f"     ğŸ’¬ ëŒ“ê¸€ {comment_count}ê°œ ìˆ˜ì§‘")
                
                time.sleep(2)  # ìš”ì²­ ê°„ ëŒ€ê¸°
        
        # ë°ì´í„° ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 7ì›” ì „ì²´ ê²Œì‹œê¸€ ì €ì¥
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # CSV ì €ì¥
        df = pd.DataFrame(all_july_posts)
        csv_file = f"data/july_2025_free_board_{timestamp}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ ì „ì²´ ê²Œì‹œê¸€ CSV: {csv_file}")
        
        # JSON ì €ì¥
        json_file = f"data/july_2025_free_board_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(all_july_posts, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ ì „ì²´ ê²Œì‹œê¸€ JSON: {json_file}")
        
        # ëŒ“ê¸€ í¬í•¨ ìƒì„¸ ì •ë³´ ì €ì¥
        if july_detailed_posts:
            detailed_csv = f"data/july_2025_free_board_detailed_{timestamp}.csv"
            detailed_df = pd.DataFrame([
                {
                    'title': p.get('title', ''),
                    'content': p.get('content', ''),
                    'full_content': p.get('full_content', ''),
                    'author': p.get('author', ''),
                    'created_time': p.get('created_time', ''),
                    'parsed_datetime': parse_everytime_time(p.get('created_time', '')),
                    'comment_count': p.get('comment_count', ''),
                    'detailed_comment_count': p.get('detailed_comment_count', 0),
                    'post_link': p.get('post_link', ''),
                    'comments_json': json.dumps(p.get('comments', []), ensure_ascii=False),
                    'board_id': 'free',
                    'collected_at': p.get('collected_at', '')
                } for p in july_detailed_posts
            ])
            detailed_df.to_csv(detailed_csv, index=False, encoding='utf-8-sig')
            print(f"ğŸ“„ ìƒì„¸ì •ë³´ CSV: {detailed_csv}")
            
            detailed_json = f"data/july_2025_free_board_detailed_{timestamp}.json"
            with open(detailed_json, 'w', encoding='utf-8') as f:
                json.dump(july_detailed_posts, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ ìƒì„¸ì •ë³´ JSON: {detailed_json}")
        
        # í†µê³„ ì •ë³´ ìƒì„±
        total_comments = sum(len(p.get('comments', [])) for p in july_detailed_posts)
        
        # ì¼ë³„ ê²Œì‹œê¸€ ìˆ˜ í†µê³„
        daily_stats = {}
        for post in all_july_posts:
            parsed_time = parse_everytime_time(post.get('created_time', ''))
            if parsed_time:
                date_key = parsed_time.strftime('%Y-%m-%d')
                daily_stats[date_key] = daily_stats.get(date_key, 0) + 1
        
        # ìš”ì•½ ì •ë³´
        summary = {
            'crawl_info': {
                'target_period': '2025ë…„ 7ì›”',
                'board_name': 'ì„±ë‚¨ìº  ììœ ê²Œì‹œíŒ',
                'crawl_completed_at': datetime.now().isoformat(),
                'pages_crawled': current_page - 1
            },
            'statistics': {
                'total_july_posts': len(all_july_posts),
                'posts_with_comments': len(posts_with_comments),
                'detailed_posts_crawled': len(july_detailed_posts),
                'total_comments_collected': total_comments,
                'daily_post_count': daily_stats
            },
            'files_created': {
                'all_posts_csv': csv_file,
                'all_posts_json': json_file,
                'detailed_csv': detailed_csv if july_detailed_posts else None,
                'detailed_json': detailed_json if july_detailed_posts else None
            }
        }
        
        summary_file = f"data/july_2025_free_board_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ í¬ë¡¤ë§ ìš”ì•½: {summary_file}")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ‰ 2025ë…„ 7ì›” ììœ ê²Œì‹œíŒ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"   - ì´ ê²Œì‹œê¸€: {len(all_july_posts)}ê°œ")
        print(f"   - ëŒ“ê¸€ ìˆëŠ” ê²Œì‹œê¸€: {len(posts_with_comments)}ê°œ")
        print(f"   - ìƒì„¸ ìˆ˜ì§‘ ê²Œì‹œê¸€: {len(july_detailed_posts)}ê°œ")
        print(f"   - ì´ ìˆ˜ì§‘ ëŒ“ê¸€: {total_comments}ê°œ")
        print(f"   - í¬ë¡¤ë§ í˜ì´ì§€: {current_page - 1}í˜ì´ì§€")
        
        print(f"\nğŸ“… ì¼ë³„ ê²Œì‹œê¸€ ìˆ˜:")
        for date, count in sorted(daily_stats.items()):
            print(f"   - {date}: {count}ê°œ")
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if crawler.driver:
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ...")
            crawler.quit()
        
        print("\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    crawl_july_2025_free_board()
