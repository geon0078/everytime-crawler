"""
ëŒ€ëŸ‰ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ë„êµ¬
"""

import os
import json
import pandas as pd
import glob
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re


class MassiveCrawlingAnalyzer:
    """ëŒ€ëŸ‰ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir="data"):
        self.data_dir = data_dir
        self.crawling_data = {}
        self.summary_data = None
        
    def load_crawling_data(self):
        """í¬ë¡¤ë§ëœ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ í¬ë¡¤ë§ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # JSON íŒŒì¼ë“¤ ì°¾ê¸°
        json_files = glob.glob(os.path.join(self.data_dir, "massive_crawl_*.json"))
        summary_files = glob.glob(os.path.join(self.data_dir, "massive_crawl_summary_*.json"))
        
        print(f"   ë°ì´í„° íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬")
        print(f"   ìš”ì•½ íŒŒì¼ {len(summary_files)}ê°œ ë°œê²¬")
        
        # ê²Œì‹œíŒë³„ ë°ì´í„° ë¡œë“œ
        for json_file in json_files:
            filename = os.path.basename(json_file)
            
            # íŒŒì¼ëª…ì—ì„œ ê²Œì‹œíŒ ID ì¶”ì¶œ
            match = re.search(r'massive_crawl_(\w+)_\d+\.json', filename)
            if match:
                board_id = match.group(1)
                
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    self.crawling_data[board_id] = data
                    print(f"   âœ… {board_id}: {len(data)}ê°œ ê²Œì‹œê¸€ ë¡œë“œ")
                    
                except Exception as e:
                    print(f"   âŒ {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ìš”ì•½ ë°ì´í„° ë¡œë“œ (ê°€ì¥ ìµœì‹  ê²ƒ)
        if summary_files:
            latest_summary = max(summary_files, key=os.path.getctime)
            try:
                with open(latest_summary, 'r', encoding='utf-8') as f:
                    self.summary_data = json.load(f)
                print(f"   ğŸ“Š ìš”ì•½ ë°ì´í„° ë¡œë“œ: {os.path.basename(latest_summary)}")
            except Exception as e:
                print(f"   âŒ ìš”ì•½ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return len(self.crawling_data) > 0
    
    def generate_overall_statistics(self):
        """ì „ì²´ í†µê³„ ìƒì„±"""
        print("\nğŸ“Š ì „ì²´ í†µê³„ ë¶„ì„")
        print("=" * 50)
        
        if not self.crawling_data:
            print("âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        total_posts = sum(len(posts) for posts in self.crawling_data.values())
        total_boards = len(self.crawling_data)
        
        print(f"ğŸ“‹ ë¶„ì„ëœ ê²Œì‹œíŒ ìˆ˜: {total_boards}ê°œ")
        print(f"ğŸ“ ì´ ê²Œì‹œê¸€ ìˆ˜: {total_posts:,}ê°œ")
        
        # ê²Œì‹œíŒë³„ ê²Œì‹œê¸€ ìˆ˜
        print(f"\nğŸ“‹ ê²Œì‹œíŒë³„ ê²Œì‹œê¸€ ìˆ˜:")
        board_counts = {}
        for board_id, posts in self.crawling_data.items():
            count = len(posts)
            board_counts[board_id] = count
            print(f"   {board_id}: {count:,}ê°œ")
        
        # ê°€ì¥ í™œë°œí•œ ê²Œì‹œíŒ
        most_active_board = max(board_counts, key=board_counts.get)
        print(f"\nğŸ”¥ ê°€ì¥ í™œë°œí•œ ê²Œì‹œíŒ: {most_active_board} ({board_counts[most_active_board]:,}ê°œ)")
        
        return {
            'total_posts': total_posts,
            'total_boards': total_boards,
            'board_counts': board_counts,
            'most_active_board': most_active_board
        }
    
    def analyze_posting_patterns(self):
        """ê²Œì‹œê¸€ ì‘ì„± íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ“ˆ ê²Œì‹œê¸€ ì‘ì„± íŒ¨í„´ ë¶„ì„")
        print("=" * 50)
        
        all_posts = []
        for board_id, posts in self.crawling_data.items():
            for post in posts:
                post['board_id'] = board_id
                all_posts.append(post)
        
        if not all_posts:
            print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(all_posts)
        
        print(f"ğŸ“Š ì´ ë¶„ì„ ëŒ€ìƒ: {len(df)}ê°œ ê²Œì‹œê¸€")
        
        # ì‘ì„±ì ë¶„ì„
        if 'author' in df.columns:
            author_counts = df['author'].value_counts()
            print(f"\nâœï¸ ìƒìœ„ ì‘ì„±ì (TOP 10):")
            for i, (author, count) in enumerate(author_counts.head(10).items(), 1):
                print(f"   {i:2d}. {author}: {count}ê°œ")
        
        # ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        if 'comment_count' in df.columns:
            df['comment_count'] = pd.to_numeric(df['comment_count'], errors='coerce').fillna(0)
            
            avg_comments = df['comment_count'].mean()
            max_comments = df['comment_count'].max()
            
            print(f"\nğŸ’¬ ëŒ“ê¸€ í†µê³„:")
            print(f"   í‰ê·  ëŒ“ê¸€ ìˆ˜: {avg_comments:.1f}ê°œ")
            print(f"   ìµœëŒ€ ëŒ“ê¸€ ìˆ˜: {max_comments}ê°œ")
            
            # ëŒ“ê¸€ì´ ë§ì€ ê²Œì‹œê¸€
            top_commented = df.nlargest(5, 'comment_count')
            print(f"\nğŸ”¥ ëŒ“ê¸€ ë§ì€ ê²Œì‹œê¸€ (TOP 5):")
            for i, (_, post) in enumerate(top_commented.iterrows(), 1):
                title = post.get('title', 'N/A')[:50]
                comments = post.get('comment_count', 0)
                board = post.get('board_id', 'N/A')
                print(f"   {i}. [{board}] {title}... ({comments}ê°œ)")
        
        # ì¡°íšŒìˆ˜ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
        if 'view_count' in df.columns:
            df['view_count'] = pd.to_numeric(df['view_count'], errors='coerce').fillna(0)
            
            avg_views = df['view_count'].mean()
            max_views = df['view_count'].max()
            
            print(f"\nğŸ‘ï¸ ì¡°íšŒìˆ˜ í†µê³„:")
            print(f"   í‰ê·  ì¡°íšŒìˆ˜: {avg_views:.1f}íšŒ")
            print(f"   ìµœëŒ€ ì¡°íšŒìˆ˜: {max_views}íšŒ")
        
        return df
    
    def analyze_content_trends(self):
        """ì½˜í…ì¸  íŠ¸ë Œë“œ ë¶„ì„"""
        print("\nğŸ“ ì½˜í…ì¸  íŠ¸ë Œë“œ ë¶„ì„")
        print("=" * 50)
        
        all_titles = []
        for board_id, posts in self.crawling_data.items():
            for post in posts:
                title = post.get('title', '')
                if title:
                    all_titles.append(title)
        
        if not all_titles:
            print("âŒ ë¶„ì„í•  ì œëª© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ì œëª©: {len(all_titles)}ê°œ")
        
        # ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ)
        all_text = ' '.join(all_titles)
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        korean_words = re.findall(r'[ê°€-í£]{2,}', all_text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'ì´ë²ˆ', 'ì €ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ê·¸ëƒ¥', 'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ë„ˆë¬´', 'ì—„ì²­', 'ë˜ê²Œ', 'ì¢€'}
        korean_words = [word for word in korean_words if word not in stop_words]
        
        word_counts = Counter(korean_words)
        
        print(f"\nğŸ” ì¸ê¸° í‚¤ì›Œë“œ (TOP 20):")
        for i, (word, count) in enumerate(word_counts.most_common(20), 1):
            print(f"   {i:2d}. {word}: {count}íšŒ")
        
        # ê²Œì‹œíŒë³„ ì¸ê¸° í‚¤ì›Œë“œ
        print(f"\nğŸ“‹ ê²Œì‹œíŒë³„ ì¸ê¸° í‚¤ì›Œë“œ:")
        for board_id, posts in self.crawling_data.items():
            board_titles = [post.get('title', '') for post in posts]
            board_text = ' '.join(board_titles)
            board_words = re.findall(r'[ê°€-í£]{2,}', board_text)
            board_words = [word for word in board_words if word not in stop_words]
            
            if board_words:
                board_word_counts = Counter(board_words)
                top_words = board_word_counts.most_common(5)
                word_str = ', '.join([f"{word}({count})" for word, count in top_words])
                print(f"   {board_id}: {word_str}")
        
        return word_counts
    
    def generate_visualizations(self):
        """ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“Š ì‹œê°í™” ìƒì„±")
        print("=" * 50)
        
        try:
            # ê²Œì‹œíŒë³„ ê²Œì‹œê¸€ ìˆ˜ ì°¨íŠ¸
            board_counts = {board_id: len(posts) for board_id, posts in self.crawling_data.items()}
            
            plt.figure(figsize=(12, 8))
            
            # 1. ê²Œì‹œíŒë³„ ê²Œì‹œê¸€ ìˆ˜
            plt.subplot(2, 2, 1)
            boards = list(board_counts.keys())
            counts = list(board_counts.values())
            
            plt.bar(boards, counts, color='skyblue')
            plt.title('ê²Œì‹œíŒë³„ ê²Œì‹œê¸€ ìˆ˜')
            plt.xlabel('ê²Œì‹œíŒ')
            plt.ylabel('ê²Œì‹œê¸€ ìˆ˜')
            plt.xticks(rotation=45)
            
            # 2. ê²Œì‹œíŒë³„ ë¹„ìœ¨ (íŒŒì´ ì°¨íŠ¸)
            plt.subplot(2, 2, 2)
            plt.pie(counts, labels=boards, autopct='%1.1f%%')
            plt.title('ê²Œì‹œíŒë³„ ê²Œì‹œê¸€ ë¹„ìœ¨')
            
            # 3. ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ëª¨ë“  ê²Œì‹œê¸€)
            all_comments = []
            for posts in self.crawling_data.values():
                for post in posts:
                    comment_count = post.get('comment_count', '0')
                    try:
                        all_comments.append(int(comment_count))
                    except:
                        all_comments.append(0)
            
            if all_comments:
                plt.subplot(2, 2, 3)
                plt.hist(all_comments, bins=20, color='lightgreen', alpha=0.7)
                plt.title('ëŒ“ê¸€ ìˆ˜ ë¶„í¬')
                plt.xlabel('ëŒ“ê¸€ ìˆ˜')
                plt.ylabel('ê²Œì‹œê¸€ ìˆ˜')
            
            # 4. ê²Œì‹œíŒë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜
            plt.subplot(2, 2, 4)
            board_avg_comments = {}
            
            for board_id, posts in self.crawling_data.items():
                comments = []
                for post in posts:
                    comment_count = post.get('comment_count', '0')
                    try:
                        comments.append(int(comment_count))
                    except:
                        comments.append(0)
                
                if comments:
                    board_avg_comments[board_id] = sum(comments) / len(comments)
            
            if board_avg_comments:
                boards = list(board_avg_comments.keys())
                avg_comments = list(board_avg_comments.values())
                
                plt.bar(boards, avg_comments, color='orange')
                plt.title('ê²Œì‹œíŒë³„ í‰ê·  ëŒ“ê¸€ ìˆ˜')
                plt.xlabel('ê²Œì‹œíŒ')
                plt.ylabel('í‰ê·  ëŒ“ê¸€ ìˆ˜')
                plt.xticks(rotation=45)
            
            plt.tight_layout()
            
            # ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            chart_filename = f"data/analysis_charts_{timestamp}.png"
            plt.savefig(chart_filename, dpi=300, bbox_inches='tight')
            
            print(f"ğŸ“Š ì°¨íŠ¸ ì €ì¥: {chart_filename}")
            plt.show()
            
        except ImportError:
            print("âš ï¸ matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   pip install matplotlib seaborn ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def generate_analysis_report(self):
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“‹ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±")
        print("=" * 50)
        
        # ë¶„ì„ ì‹¤í–‰
        overall_stats = self.generate_overall_statistics()
        df = self.analyze_posting_patterns()
        word_counts = self.analyze_content_trends()
        
        # ë³´ê³ ì„œ ìƒì„±
        report = {
            'analysis_info': {
                'generated_at': datetime.now().isoformat(),
                'data_source': 'massive board crawling',
                'analyzer_version': '1.0.0'
            },
            'overall_statistics': overall_stats,
            'content_trends': {
                'top_keywords': word_counts.most_common(50) if word_counts else [],
                'total_unique_words': len(word_counts) if word_counts else 0
            }
        }
        
        # DataFrame í†µê³„ ì¶”ê°€
        if df is not None and not df.empty:
            report['posting_patterns'] = {
                'total_posts_analyzed': len(df),
                'unique_authors': df['author'].nunique() if 'author' in df.columns else 0,
                'average_comments': float(df['comment_count'].mean()) if 'comment_count' in df.columns else 0,
                'max_comments': int(df['comment_count'].max()) if 'comment_count' in df.columns else 0
            }
        
        # ìš”ì•½ ë°ì´í„° ì¶”ê°€
        if self.summary_data:
            report['crawling_summary'] = self.summary_data
        
        # ë³´ê³ ì„œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"data/analysis_report_{timestamp}.json"
        
        try:
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {report_filename}")
            
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìš”ì•½ë„ ìƒì„±
            summary_filename = f"data/analysis_summary_{timestamp}.txt"
            with open(summary_filename, 'w', encoding='utf-8') as f:
                f.write("ì—ë¸Œë¦¬íƒ€ì„ ëŒ€ëŸ‰ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ìš”ì•½\n")
                f.write("=" * 50 + "\n\n")
                
                if overall_stats:
                    f.write(f"ì´ ê²Œì‹œíŒ ìˆ˜: {overall_stats['total_boards']}ê°œ\n")
                    f.write(f"ì´ ê²Œì‹œê¸€ ìˆ˜: {overall_stats['total_posts']:,}ê°œ\n")
                    f.write(f"ê°€ì¥ í™œë°œí•œ ê²Œì‹œíŒ: {overall_stats['most_active_board']}\n\n")
                
                if word_counts:
                    f.write("ì¸ê¸° í‚¤ì›Œë“œ TOP 10:\n")
                    for i, (word, count) in enumerate(word_counts.most_common(10), 1):
                        f.write(f"{i:2d}. {word}: {count}íšŒ\n")
            
            print(f"ğŸ“ ìš”ì•½ íŒŒì¼ ì €ì¥: {summary_filename}")
            
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š ì—ë¸Œë¦¬íƒ€ì„ ëŒ€ëŸ‰ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„")
    print("=" * 60)
    
    # ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    analyzer = MassiveCrawlingAnalyzer()
    
    # ë°ì´í„° ë¡œë“œ
    if not analyzer.load_crawling_data():
        print("âŒ ë¶„ì„í•  í¬ë¡¤ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € massive_board_crawling.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
        return
    
    try:
        # ì¢…í•© ë¶„ì„ ì‹¤í–‰
        analyzer.generate_analysis_report()
        
        # ì‹œê°í™” ìƒì„± (ì„ íƒì‚¬í•­)
        create_charts = input("\nì‹œê°í™” ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if create_charts in ['y', 'yes']:
            analyzer.generate_visualizations()
        
        print("\nâœ… ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“‚ ê²°ê³¼ íŒŒì¼ë“¤ì´ data/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
