#!/usr/bin/env python3
"""
ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ HTML êµ¬ì¡° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from src.everytime_crawler.crawler import EverytimeCrawler
from bs4 import BeautifulSoup
import time


def analyze_board_html():
    """ê²Œì‹œíŒ HTML êµ¬ì¡° ë¶„ì„"""
    print("ğŸ” ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ HTML êµ¬ì¡° ë¶„ì„")
    print("=" * 50)
    
    crawler = EverytimeCrawler()
    
    try:
        print("ğŸ”§ WebDriver ì„¤ì • ì¤‘...")
        crawler.setup_driver(headless=False)
        
        print("ğŸ” ë¡œê·¸ì¸ ì‹œë„...")
        if not crawler.login():
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨!")
            return
        
        print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
        
        # ììœ ê²Œì‹œíŒ ë¶„ì„
        board_url = "https://everytime.kr/387605"
        print(f"\nğŸ“‹ ê²Œì‹œíŒ í˜ì´ì§€ ë¶„ì„: {board_url}")
        
        crawler.driver.get(board_url)
        time.sleep(5)
        
        # í˜ì´ì§€ HTML ì €ì¥
        html_content = crawler.driver.page_source
        with open("debug/board_html_analysis.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        print("ğŸ’¾ HTML ë‚´ìš©ì„ debug/board_html_analysis.htmlì— ì €ì¥")
        
        # BeautifulSoupìœ¼ë¡œ ë¶„ì„
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ê°€ëŠ¥í•œ ê²Œì‹œê¸€ ìš”ì†Œë“¤ ì°¾ê¸°
        print("\nğŸ” ê²Œì‹œê¸€ ìš”ì†Œ ë¶„ì„:")
        
        # article íƒœê·¸ ë¶„ì„
        articles = soup.find_all('article')
        print(f"ğŸ“„ article íƒœê·¸: {len(articles)}ê°œ")
        if articles:
            first_article = articles[0]
            print("ì²« ë²ˆì§¸ articleì˜ í´ë˜ìŠ¤:", first_article.get('class'))
            print("ì²« ë²ˆì§¸ article ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
            print(first_article.prettify()[:500])
            
        # a íƒœê·¸ (ë§í¬) ë¶„ì„
        links = soup.find_all('a', href=True)
        board_links = [link for link in links if '/v/' in link.get('href', '')]
        print(f"\nğŸ”— ê²Œì‹œê¸€ ë§í¬: {len(board_links)}ê°œ")
        if board_links:
            print("ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë§í¬:")
            print(board_links[0].prettify()[:300])
            
        # ì œëª©ì´ í¬í•¨ë  ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
        print("\nğŸ“ ì œëª© ìš”ì†Œ ë¶„ì„:")
        potential_titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        print(f"ì œëª© íƒœê·¸ë“¤: {len(potential_titles)}ê°œ")
        for i, title in enumerate(potential_titles[:5]):
            print(f"  {i+1}. {title.name}: {title.get_text(strip=True)[:50]}")
            
        # í´ë˜ìŠ¤ëª… ë¶„ì„
        print("\nğŸ·ï¸ ì£¼ìš” í´ë˜ìŠ¤ëª… ë¶„ì„:")
        all_elements = soup.find_all(class_=True)
        class_names = set()
        for elem in all_elements:
            classes = elem.get('class', [])
            class_names.update(classes)
        
        relevant_classes = [cls for cls in class_names if any(keyword in cls.lower() 
                          for keyword in ['title', 'subject', 'article', 'post', 'content', 'text'])]
        print("ê´€ë ¨ í´ë˜ìŠ¤ëª…ë“¤:", sorted(relevant_classes))
        
        print("\nâœ… HTML êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if crawler.driver:
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ...")
            crawler.close()


if __name__ == "__main__":
    analyze_board_html()
