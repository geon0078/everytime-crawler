"""
ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ êµ¬ì¡° ë¶„ì„ ë„êµ¬
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from everytime_crawler import EverytimeCrawler
from bs4 import BeautifulSoup
import time
from datetime import datetime


def analyze_board_structure(board_id="free"):
    """
    ê²Œì‹œíŒ HTML êµ¬ì¡° ë¶„ì„
    
    Args:
        board_id (str): ë¶„ì„í•  ê²Œì‹œíŒ ID
    """
    print(f"ğŸ” '{board_id}' ê²Œì‹œíŒ êµ¬ì¡° ë¶„ì„ ì‹œì‘...")
    
    crawler = EverytimeCrawler()
    
    try:
        # WebDriver ì„¤ì •
        crawler.setup_driver(headless=False)
        
        # ë¡œê·¸ì¸
        if crawler.login():
            print("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            
            # ê²Œì‹œíŒ í˜ì´ì§€ë¡œ ì´ë™
            board_url = f"{crawler.base_url}/{board_id}"
            crawler.driver.get(board_url)
            time.sleep(3)
            
            print(f"ğŸ“ í˜„ì¬ URL: {crawler.driver.current_url}")
            
            # HTML ì†ŒìŠ¤ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_file = f"debug/board_{board_id}_structure_{timestamp}.html"
            
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(crawler.driver.page_source)
            
            print(f"ğŸ’¾ HTML ì†ŒìŠ¤ ì €ì¥: {debug_file}")
            
            # BeautifulSoupìœ¼ë¡œ êµ¬ì¡° ë¶„ì„
            soup = BeautifulSoup(crawler.driver.page_source, 'html.parser')
            
            print("\nğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„:")
            
            # 1. ì „ì²´ í˜ì´ì§€ ì •ë³´
            title = soup.find('title')
            print(f"   í˜ì´ì§€ ì œëª©: {title.text if title else 'N/A'}")
            
            # 2. ì£¼ìš” ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            print("\nğŸ“¦ ì£¼ìš” ì»¨í…Œì´ë„ˆ:")
            container_selectors = [
                'main', '.main', '#main',
                'content', '.content', '#content',
                'container', '.container', '#container',
                'wrapper', '.wrapper', '#wrapper'
            ]
            
            for selector in container_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"   {selector}: {len(elements)}ê°œ ë°œê²¬")
            
            # 3. ê²Œì‹œê¸€ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°
            print("\nğŸ“ ê²Œì‹œê¸€ ê´€ë ¨ ìš”ì†Œ:")
            post_selectors = [
                'article', '.article',
                '.post', '.board-item', '.list-item',
                'tr.list', 'li',
                '.content-item', '.item'
            ]
            
            for selector in post_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"   {selector}: {len(elements)}ê°œ ë°œê²¬")
                    
                    # ì²« ë²ˆì§¸ ìš”ì†Œì˜ êµ¬ì¡° ë¶„ì„
                    if elements:
                        first_element = elements[0]
                        print(f"     ì²« ë²ˆì§¸ ìš”ì†Œ í´ë˜ìŠ¤: {first_element.get('class', [])}")
                        print(f"     ì²« ë²ˆì§¸ ìš”ì†Œ ID: {first_element.get('id', 'N/A')}")
                        
                        # í…ìŠ¤íŠ¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                        text_content = first_element.get_text(strip=True)[:100]
                        print(f"     í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text_content}...")
            
            # 4. ë§í¬ ìš”ì†Œ ë¶„ì„
            print("\nğŸ”— ë§í¬ ìš”ì†Œ:")
            links = soup.find_all('a', href=True)
            
            view_links = [link for link in links if 'view' in link.get('href', '')]
            post_links = [link for link in links if any(keyword in link.get('href', '') 
                         for keyword in ['board', 'post', 'article'])]
            
            print(f"   ì „ì²´ ë§í¬: {len(links)}ê°œ")
            print(f"   'view' í¬í•¨ ë§í¬: {len(view_links)}ê°œ")
            print(f"   ê²Œì‹œê¸€ ê´€ë ¨ ë§í¬: {len(post_links)}ê°œ")
            
            if view_links:
                print("   View ë§í¬ ìƒ˜í”Œ:")
                for i, link in enumerate(view_links[:3]):
                    href = link.get('href')
                    text = link.get_text(strip=True)[:50]
                    print(f"     {i+1}. {href} - {text}")
            
            # 5. í¼ ìš”ì†Œ ë¶„ì„
            print("\nğŸ“‹ í¼ ìš”ì†Œ:")
            forms = soup.find_all('form')
            print(f"   í¼ ê°œìˆ˜: {len(forms)}")
            
            for i, form in enumerate(forms):
                action = form.get('action', 'N/A')
                method = form.get('method', 'N/A')
                print(f"     í¼ {i+1}: action={action}, method={method}")
            
            # 6. JavaScript ê´€ë ¨ ìš”ì†Œ
            print("\nâš™ï¸ JavaScript ê´€ë ¨:")
            scripts = soup.find_all('script')
            print(f"   ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸: {len(scripts)}ê°œ")
            
            # Reactë‚˜ Vue ë“± í”„ë ˆì„ì›Œí¬ ì‚¬ìš© ì—¬ë¶€
            page_source = crawler.driver.page_source.lower()
            frameworks = ['react', 'vue', 'angular', 'jquery']
            
            for framework in frameworks:
                if framework in page_source:
                    print(f"   {framework.capitalize()} ì‚¬ìš© ê°€ëŠ¥ì„± ê°ì§€")
            
            # 7. CSS í´ë˜ìŠ¤ ë¶„ì„
            print("\nğŸ¨ CSS í´ë˜ìŠ¤ ë¶„ì„:")
            all_elements = soup.find_all(class_=True)
            all_classes = []
            
            for element in all_elements:
                classes = element.get('class', [])
                all_classes.extend(classes)
            
            from collections import Counter
            class_counts = Counter(all_classes)
            most_common_classes = class_counts.most_common(10)
            
            print("   ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤:")
            for class_name, count in most_common_classes:
                print(f"     .{class_name}: {count}íšŒ")
            
            # 8. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„
            print("\nğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜:")
            pagination_selectors = [
                '.pagination', '.paging', '.page',
                'nav', '.nav', '.navigation'
            ]
            
            for selector in pagination_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"   {selector}: {len(elements)}ê°œ ë°œê²¬")
            
            # í˜ì´ì§€ ë²ˆí˜¸ ë§í¬ ì°¾ê¸°
            page_links = soup.find_all('a', href=lambda x: x and 'page=' in x)
            print(f"   í˜ì´ì§€ ë§í¬: {len(page_links)}ê°œ")
            
            # 9. ê²Œì‹œê¸€ ìš”ì†Œì˜ ìƒì„¸ êµ¬ì¡° ë¶„ì„
            print("\nğŸ”¬ ê²Œì‹œê¸€ ìš”ì†Œ ìƒì„¸ ë¶„ì„:")
            
            # ê°€ì¥ ìœ ë ¥í•œ ê²Œì‹œê¸€ ì…€ë ‰í„° ì°¾ê¸°
            article_elements = soup.select('article.list')
            if not article_elements:
                article_elements = soup.select('.article')
            if not article_elements:
                article_elements = soup.select('tr.list')
            
            if article_elements:
                print(f"   ê²Œì‹œê¸€ ìš”ì†Œ {len(article_elements)}ê°œ ë°œê²¬")
                
                # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ì˜ ìƒì„¸ êµ¬ì¡°
                first_article = article_elements[0]
                print("\n   ì²« ë²ˆì§¸ ê²Œì‹œê¸€ êµ¬ì¡°:")
                
                # ì œëª© ìš”ì†Œ ì°¾ê¸°
                title_candidates = [
                    first_article.select_one('.title'),
                    first_article.select_one('.subject'),
                    first_article.select_one('h3'),
                    first_article.select_one('h4'),
                    first_article.select_one('a[href*="view"]')
                ]
                
                for i, candidate in enumerate(title_candidates):
                    if candidate:
                        print(f"     ì œëª© í›„ë³´ {i+1}: {candidate.get_text(strip=True)[:50]}")
                
                # ì‘ì„±ì ìš”ì†Œ ì°¾ê¸°
                author_candidates = [
                    first_article.select_one('.writer'),
                    first_article.select_one('.author'),
                    first_article.select_one('.nickname'),
                    first_article.select_one('.user')
                ]
                
                for i, candidate in enumerate(author_candidates):
                    if candidate:
                        print(f"     ì‘ì„±ì í›„ë³´ {i+1}: {candidate.get_text(strip=True)}")
                
                # ì‹œê°„ ìš”ì†Œ ì°¾ê¸°
                time_candidates = [
                    first_article.select_one('.time'),
                    first_article.select_one('.date'),
                    first_article.select_one('.created_at')
                ]
                
                for i, candidate in enumerate(time_candidates):
                    if candidate:
                        print(f"     ì‹œê°„ í›„ë³´ {i+1}: {candidate.get_text(strip=True)}")
            
            print(f"\nâœ… êµ¬ì¡° ë¶„ì„ ì™„ë£Œ! ìƒì„¸ ê²°ê³¼ëŠ” {debug_file}ì„ í™•ì¸í•˜ì„¸ìš”.")
            
        else:
            print("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨!")
            
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if crawler.driver:
            crawler.quit()


def analyze_multiple_boards():
    """ì—¬ëŸ¬ ê²Œì‹œíŒ êµ¬ì¡° ë¹„êµ ë¶„ì„"""
    boards = ['free', 'secret', 'freshman']
    
    print("ğŸ” ì—¬ëŸ¬ ê²Œì‹œíŒ êµ¬ì¡° ë¹„êµ ë¶„ì„")
    print("=" * 50)
    
    for board_id in boards:
        print(f"\nğŸ“‹ {board_id} ê²Œì‹œíŒ ë¶„ì„...")
        analyze_board_structure(board_id)
        time.sleep(5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€


if __name__ == "__main__":
    print("ğŸ”§ ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ êµ¬ì¡° ë¶„ì„ ë„êµ¬")
    print("=" * 50)
    
    # ë‹¨ì¼ ê²Œì‹œíŒ ë¶„ì„
    analyze_board_structure("free")
    
    # ì—¬ëŸ¬ ê²Œì‹œíŒ ë¹„êµ ë¶„ì„ (ì„ íƒì‚¬í•­)
    # analyze_multiple_boards()
