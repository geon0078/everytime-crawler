"""
ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ í¬ë¡¤ë§ ì „ìš© ëª¨ë“ˆ
"""

import time
import json
from datetime import datetime
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup


class BoardCrawler:
    """ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ í¬ë¡¤ë§ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, crawler_instance):
        """
        BoardCrawler ì´ˆê¸°í™”
        
        Args:
            crawler_instance: EverytimeCrawler ì¸ìŠ¤í„´ìŠ¤
        """
        self.crawler = crawler_instance
        self.driver = crawler_instance.driver
        self.base_url = crawler_instance.base_url
        
        # ê²Œì‹œíŒ ID ë§¤í•‘
        self.board_map = {
            "free": "ììœ ê²Œì‹œíŒ",
            "secret": "ë¹„ë°€ê²Œì‹œíŒ", 
            "freshman": "ìƒˆë‚´ê¸°ê²Œì‹œíŒ",
            "graduate": "ì¡¸ì—…ìƒê²Œì‹œíŒ",
            "job": "ì·¨ì—…ê²Œì‹œíŒ",
            "exam": "ì‹œí—˜ì •ë³´ê²Œì‹œíŒ",
            "club": "ë™ì•„ë¦¬ê²Œì‹œíŒ",
            "market": "ì¥í„°ê²Œì‹œíŒ"
        }
    
    def get_board_posts(self, board_id="free", pages=3, delay=2):
        """
        ê²Œì‹œíŒ ê¸€ ëª©ë¡ í¬ë¡¤ë§
        
        Args:
            board_id (str): ê²Œì‹œíŒ ID (free, secret, freshman ë“±)
            pages (int): í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
            delay (int): í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
            
        Returns:
            list: ê²Œì‹œê¸€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” '{self.board_map.get(board_id, board_id)}' ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘...")
        
        all_posts = []
        
        try:
            # ê²Œì‹œíŒ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            board_url = f"{self.base_url}/{board_id}"
            self.driver.get(board_url)
            time.sleep(3)
            
            print(f"ğŸ“ í˜„ì¬ URL: {self.driver.current_url}")
            
            for page in range(1, pages + 1):
                print(f"ğŸ“„ í˜ì´ì§€ {page}/{pages} í¬ë¡¤ë§ ì¤‘...")
                
                # í˜ì´ì§€ ì´ë™ (ì²« í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°)
                if page > 1:
                    page_url = f"{board_url}?page={page}"
                    self.driver.get(page_url)
                    time.sleep(delay)
                
                # í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ì¶”ì¶œ
                posts = self._extract_posts_from_page(board_id, page)
                all_posts.extend(posts)
                
                print(f"âœ… í˜ì´ì§€ {page}ì—ì„œ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘")
                
        except Exception as e:
            print(f"âŒ ê²Œì‹œíŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self._save_debug_info(board_id)
        
        print(f"ğŸ‰ ì´ {len(all_posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ!")
        return all_posts
    
    def _extract_posts_from_page(self, board_id, page_num):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ"""
        posts = []
        
        try:
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ì—ë¸Œë¦¬íƒ€ì„ ê²Œì‹œíŒ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
            post_selectors = [
                "article.list",           # ì¼ë°˜ì ì¸ ê²Œì‹œê¸€ êµ¬ì¡°
                ".article",               # ê¸°ë³¸ article í´ë˜ìŠ¤
                "tr.list",               # í…Œì´ë¸” í˜•íƒœ ê²Œì‹œíŒ
                ".board-item",           # ì»¤ìŠ¤í…€ ê²Œì‹œíŒ ì•„ì´í…œ
                ".post-item",            # í¬ìŠ¤íŠ¸ ì•„ì´í…œ
                ".content-wrapper a",    # ë§í¬ í˜•íƒœ ê²Œì‹œê¸€
                ".list-item"             # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ
            ]
            
            post_elements = []
            used_selector = None
            
            for selector in post_selectors:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    post_elements = elements
                    used_selector = selector
                    print(f"âœ… '{selector}' ì…€ë ‰í„°ë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                    break
            
            if not post_elements:
                print("âš ï¸ ê²Œì‹œê¸€ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return posts
            
            # ê° ê²Œì‹œê¸€ì—ì„œ ì •ë³´ ì¶”ì¶œ
            for idx, element in enumerate(post_elements[:20]):  # ìƒìœ„ 20ê°œë§Œ ì²˜ë¦¬
                try:
                    post_info = self._extract_post_info(element, used_selector)
                    if post_info:
                        post_info['board_id'] = board_id
                        post_info['page'] = page_num
                        post_info['collected_at'] = datetime.now().isoformat()
                        posts.append(post_info)
                
                except Exception as e:
                    print(f"âš ï¸ ê²Œì‹œê¸€ {idx+1} ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        return posts
    
    def _extract_post_info(self, element, selector_used):
        """ê°œë³„ ê²Œì‹œê¸€ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        post_info = {}
        
        try:
            # BeautifulSoupìœ¼ë¡œ ë” ì •í™•í•œ íŒŒì‹±
            soup = BeautifulSoup(element.get_attribute('outerHTML'), 'html.parser')
            
            # ì œëª© ì¶”ì¶œ - ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
            title_selectors = [
                '.title',
                '.subject', 
                'h3',
                'h4',
                '.article-title',
                '.post-title',
                'a[href*="view"]',
                '.text'
            ]
            
            title = None
            for title_sel in title_selectors:
                title_elem = soup.select_one(title_sel)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title and len(title) > 2:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                        break
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_selectors = [
                '.writer',
                '.author',
                '.nickname',
                '.user',
                '.name'
            ]
            
            author = "ìµëª…"
            for author_sel in author_selectors:
                author_elem = soup.select_one(author_sel)
                if author_elem:
                    author = author_elem.get_text(strip=True)
                    if author:
                        break
            
            # ì‘ì„±ì‹œê°„ ì¶”ì¶œ
            time_selectors = [
                '.time',
                '.date',
                '.created_at',
                '.timestamp',
                '.datetime'
            ]
            
            created_time = None
            for time_sel in time_selectors:
                time_elem = soup.select_one(time_sel)
                if time_elem:
                    created_time = time_elem.get_text(strip=True)
                    if created_time:
                        break
            
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            comment_selectors = [
                '.comment',
                '.reply',
                '.comment-count',
                '.reply-count'
            ]
            
            comment_count = "0"
            for comment_sel in comment_selectors:
                comment_elem = soup.select_one(comment_sel)
                if comment_elem:
                    comment_text = comment_elem.get_text(strip=True)
                    # ìˆ«ìë§Œ ì¶”ì¶œ
                    import re
                    numbers = re.findall(r'\d+', comment_text)
                    if numbers:
                        comment_count = numbers[0]
                        break
            
            # ê²Œì‹œê¸€ ë§í¬ ì¶”ì¶œ
            link_elem = soup.select_one('a[href]')
            post_link = None
            if link_elem:
                href = link_elem.get('href')
                if href:
                    if href.startswith('/'):
                        post_link = f"{self.base_url}{href}"
                    else:
                        post_link = href
            
            # ì¡°íšŒìˆ˜ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
            view_selectors = [
                '.view',
                '.views',
                '.hit',
                '.read-count'
            ]
            
            view_count = None
            for view_sel in view_selectors:
                view_elem = soup.select_one(view_sel)
                if view_elem:
                    view_text = view_elem.get_text(strip=True)
                    import re
                    numbers = re.findall(r'\d+', view_text)
                    if numbers:
                        view_count = numbers[0]
                        break
            
            # ìµœì†Œí•œ ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ ë°˜í™˜
            if title and len(title) > 1:
                post_info = {
                    'title': title,
                    'author': author,
                    'created_time': created_time,
                    'comment_count': comment_count,
                    'view_count': view_count,
                    'post_link': post_link,
                    'selector_used': selector_used
                }
                
                return post_info
        
        except Exception as e:
            print(f"âš ï¸ ê²Œì‹œê¸€ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return None
    
    def get_post_detail(self, post_url):
        """
        ê°œë³„ ê²Œì‹œê¸€ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            post_url (str): ê²Œì‹œê¸€ URL
            
        Returns:
            dict: ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´
        """
        try:
            print(f"ğŸ“– ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§: {post_url}")
            
            self.driver.get(post_url)
            time.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ
            content_selectors = [
                '.content',
                '.article-content',
                '.post-content',
                '.text',
                '.body',
                'p'
            ]
            
            content = ""
            for content_sel in content_selectors:
                content_elem = soup.select_one(content_sel)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    if content and len(content) > 10:
                        break
            
            # ëŒ“ê¸€ ì¶”ì¶œ
            comments = []
            comment_selectors = [
                '.comment',
                '.reply', 
                '.comment-item',
                '.reply-item'
            ]
            
            for comment_sel in comment_selectors:
                comment_elems = soup.select(comment_sel)
                if comment_elems:
                    for comment_elem in comment_elems:
                        comment_text = comment_elem.get_text(strip=True)
                        if comment_text and len(comment_text) > 2:
                            comments.append(comment_text)
                    break
            
            detail_info = {
                'url': post_url,
                'content': content,
                'comments': comments,
                'comment_count': len(comments),
                'collected_at': datetime.now().isoformat()
            }
            
            print(f"âœ… ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ (ëŒ“ê¸€ {len(comments)}ê°œ)")
            return detail_info
            
        except Exception as e:
            print(f"âŒ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def save_posts_to_csv(self, posts, filename=None):
        """ê²Œì‹œê¸€ ëª©ë¡ì„ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not posts:
            print("âš ï¸ ì €ì¥í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            board_id = posts[0].get('board_id', 'unknown')
            filename = f"data/board_{board_id}_{timestamp}.csv"
        
        try:
            import pandas as pd
            df = pd.DataFrame(posts)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ê²Œì‹œê¸€ {len(posts)}ê°œê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def save_posts_to_json(self, posts, filename=None):
        """ê²Œì‹œê¸€ ëª©ë¡ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not posts:
            print("âš ï¸ ì €ì¥í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            board_id = posts[0].get('board_id', 'unknown')
            filename = f"data/board_{board_id}_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ê²Œì‹œê¸€ {len(posts)}ê°œê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _save_debug_info(self, board_id):
        """ë””ë²„ê¹…ì„ ìœ„í•œ í˜ì´ì§€ ì •ë³´ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_filename = f"debug/board_{board_id}_debug_{timestamp}.html"
            
            with open(debug_filename, 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            
            print(f"ğŸ”§ ë””ë²„ê·¸ ì •ë³´ê°€ '{debug_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ë””ë²„ê·¸ ì •ë³´ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
